import math
from utils.utils import END_INDEX
import numpy as np
from collections import Counter

class AccuracyMetric:
    def __init__(self, pad_idx=0):
        super().__init__()
        self.pad_idx = pad_idx
    
    def calculate(self, outputs, targets):
        #print('metric_out: ', outputs.size())
        #print('metric_trg: ', targets.size())
        batch_size, seq_len, vocab_size = outputs.size()

        outputs = outputs.view(batch_size * seq_len, vocab_size)
        targets = targets.view(batch_size * seq_len)

        predicts = outputs.argmax(dim=1)
        correct_count = predicts == targets 

        correct_count.masked_fill_((targets == self.pad_idx), 0)

        n_word_correct = correct_count.sum().item()
        # print(correct_count)
        n_word_total = (targets != self.pad_idx).sum().item()
        
        return n_word_correct, n_word_total

class BLEUMetric:
    '''
    # BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating
    # the quality of text which has been machine-translated from one 
    # natural language to another.

    # This metric is to measure the closenes of translation by finding 
    # legitimate differences in word choice and word order between 
    # the reference human translation and translation generated by 
    # the machine. Technicially, to do this evaluation, we need to 
    # compare n-gram matches between each candidate translation to 
    # the reference translations.

    ###### @n-gram: a sequence of words occurring with a given window 
    ###### where n represents the window size.

    '''

    def __init__(self):
        super().__init__()
    
    def _compute_bleu_stats(self, hypothesis, reference):
        """
        Compute statistics for BLEU.
        """
        stats = []
        stats.append(len(hypothesis))
        stats.append(len(reference))
        # 4-gram
        for n in range(1, 5):
            h_ngrams = Counter(
                [tuple(hypothesis[i:i + n]) for i in range(len(hypothesis) + 1 - n)]
            )

            r_ngrams = Counter(
                [tuple(reference[i:i + n]) for i in range(len(reference) + 1 - n)]
            )
            stats.append(max([sum((h_ngrams & r_ngrams).values()), 0]))
            stats.append(max([len(hypothesis) + 1 - n, 0]))
        return stats
    
    def _compute_bleu_score(self, stats):
        """
        Compute BLEU given n-gram statistics.

        The formula to compute BLEU score:
        BLEU = BP * exp(sigma(log p))
        with BP (Brevity Penalty) = exp(1 -r / c)
        """
        if len(list(filter(lambda x: x == 0, stats))) > 0:
            return 0
        
        # @ param c: length of candidate translation
        # @ param r: effective reference length
        (c, r) = stats[:2]
        
        log_bleu_prec = sum(
            [math.log(float(x) / y) for x, y in zip(stats[2::2], stats[3::2])]
        ) / 4.

        return math.exp(min([0, 1 - float(r) / c]) + log_bleu_prec)
    
    def get_bleu(self, hypothesis, reference):
        """
        Get validation BLEU score for dev set.
        """
        stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
        for hyp, ref in zip(hypothesis, reference):
            stats += np.array(self._compute_bleu_stats(hyp, ref))
        #print(stats)
        return 100 * self._compute_bleu_score(stats)

def preprocess(trg, out):
    END_INDEX = 3
    trg_end_pos = np.where(trg == END_INDEX)[0][0]
    trg = trg[:trg_end_pos + 1]
    out_end_pos = np.where(out == END_INDEX)
    if len(out_end_pos[0]) > 0:
        out_end_pos = out_end_pos[0][0]
        out = out[:out_end_pos + 1]
    return trg, out

if __name__ == '__main__':
    trg = np.array([5, 9, 8, 7, 7, 3, 0, 0, 0, 0])
    out = np.array([6, 8, 7, 9, 4, 4, 3, 200, 200, 200, 200])
    result = preprocess(trg, out)
    
    print(result[0])
    print(result[1])

# if __name__ == '__main__':
#     from nltk.translate.bleu_score import sentence_bleu
#     import nltk

#     metric = nltk.translate.bleu_score

#     hypothesis = [[2, 3, 4, 9], [5, 9, 7, 3]]
#     reference = [[[2, 3, 4, 9]], [[5, 9, 7, 3]]]

#     bleu_score = metric.corpus_bleu(
#                     reference, 
#                     hypothesis,
#                     smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)
#     print(bleu_score)




    