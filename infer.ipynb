{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer, BertWordPieceTokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "from src.models.model import Transformer\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "MAX_LEN = 256"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dev = 'cuda'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "weight_fn = 'weights/Transformer-En2Vi-CPE-WordPiece/best_bleu.pth'\n",
    "config = torch.load(weight_fn, map_location=dev)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data_cfg = config['config']['dataset']['train']['config']\n",
    "token_type = data_cfg['token_type']\n",
    "if token_type == 'bpe':\n",
    "    vi_tokenizer = ByteLevelBPETokenizer(\n",
    "        \"vocab/vietnamese/vietnamese-vocab.json\",\n",
    "        \"vocab/vietnamese/vietnamese-merges.txt\",\n",
    "    )\n",
    "\n",
    "    vi_tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "        (\"</s>\", vi_tokenizer.token_to_id(\"</s>\")),\n",
    "        (\"<s>\", vi_tokenizer.token_to_id(\"<s>\")),\n",
    "    )\n",
    "    vi_tokenizer.enable_truncation(max_length=MAX_LEN)\n",
    "\n",
    "    en_tokenizer = ByteLevelBPETokenizer(\n",
    "        \"vocab/english/english-vocab.json\",\n",
    "        \"vocab/english/english-merges.txt\",\n",
    "    )\n",
    "\n",
    "    en_tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "        (\"</s>\", en_tokenizer.token_to_id(\"</s>\")),\n",
    "        (\"<s>\", en_tokenizer.token_to_id(\"<s>\")),\n",
    "    )\n",
    "    en_tokenizer.enable_truncation(max_length=MAX_LEN)\n",
    "elif token_type == 'wordpiece':\n",
    "    vi_tokenizer = BertWordPieceTokenizer(\n",
    "        data_cfg['trg_vocab'],\n",
    "        lowercase=False,\n",
    "        handle_chinese_chars=True,\n",
    "        strip_accents=False,\n",
    "        cls_token='<s>',\n",
    "        pad_token='<pad>',\n",
    "        sep_token='</s>',\n",
    "        unk_token='<unk>',\n",
    "        mask_token='<mask>',\n",
    "    )\n",
    "    vi_tokenizer.enable_truncation(max_length=MAX_LEN)\n",
    "\n",
    "    en_tokenizer = BertWordPieceTokenizer(\n",
    "        data_cfg['src_vocab'],\n",
    "        lowercase=False,\n",
    "        handle_chinese_chars=True,\n",
    "        strip_accents=False,\n",
    "        cls_token='<s>',\n",
    "        pad_token='<pad>',\n",
    "        sep_token='</s>',\n",
    "        unk_token='<unk>',\n",
    "        mask_token='<mask>',\n",
    "    )\n",
    "    en_tokenizer.enable_truncation(max_length=MAX_LEN)\n",
    "\n",
    "\n",
    "TRG_EOS_TOKEN = '</s>'\n",
    "TRG_EOS_ID = vi_tokenizer.token_to_id(TRG_EOS_TOKEN)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model = Transformer(\n",
    "    n_src_vocab=en_tokenizer.get_vocab_size(),\n",
    "    n_trg_vocab=vi_tokenizer.get_vocab_size(),\n",
    "    src_pad_idx=en_tokenizer.token_to_id('<pad>'),\n",
    "    trg_pad_idx=vi_tokenizer.token_to_id('<pad>'),\n",
    "    **config['config']['model']\n",
    ").to(dev)\n",
    "model.load_state_dict(config['model_state_dict'])\n",
    "model.eval()\n",
    "print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "@torch.no_grad()\n",
    "def translate(model, src, dev, en_tokenizer, vi_tokenizer):\n",
    "    src = torch.tensor(en_tokenizer.encode(src).ids)\n",
    "    src = src.long().unsqueeze(0).to(dev)\n",
    "    pred = model.predict(src, max_length=MAX_LEN, eos_id=TRG_EOS_ID)\n",
    "    pred_ = pred.cpu().numpy()\n",
    "    pred_ = vi_tokenizer.decode_batch(pred_)[0]\n",
    "    return pred_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random string"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "src = 'Today is beautiful'\n",
    "translate(model, src, dev, en_tokenizer, vi_tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Hôm nay là một nơi tuyệt vời'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "src = 'I want to be a doctor in the future'\n",
    "translate(model, src, dev, en_tokenizer, vi_tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Tôi muốn trở thành một bác sĩ trong tương lai'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "src = 'All I want for Christmas is you'\n",
    "translate(model, src, dev, en_tokenizer, vi_tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Tất cả những gì tôi muốn cho các bạn là của tôi.'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "src = 'I want you by my side always'\n",
    "translate(model, src, dev, en_tokenizer, vi_tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Tôi luôn muốn bạn thấy mặt tôi.'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "src = 'My love is for you and you only'\n",
    "translate(model, src, dev, en_tokenizer, vi_tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Tôi dành cho các bạn và chỉ có các bạn mà thôi.'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Order importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "src = 'I want to be a doctor'\n",
    "translate(model, src, dev, en_tokenizer, vi_tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Tôi muốn trở thành một bác sĩ.'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "src = 'I a doctor want to be'\n",
    "translate(model, src, dev, en_tokenizer, vi_tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Tôi muốn một bác sĩ cần được đào tạo.'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "src = 'want to be I a doctor'\n",
    "translate(model, src, dev, en_tokenizer, vi_tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Tôi muốn trở thành bác sĩ tâm thần.'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('nlp': conda)"
  },
  "interpreter": {
   "hash": "e4dff0a35ad65f38ecea16344b491af1a0c41400e0651dd717f39521f81d3d08"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}