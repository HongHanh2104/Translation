id: Transformers-en-to-de
seed: 2104
pretrained_path: null
model:
    d_model: 512
    d_ffn: 2048
    n_layer: 6
    n_head: 8
    max_len: 256
    dropout: 0.1
trainer:
    nepochs: 100
    val_step: 1
    log_step: 30
    lr: !!float 1e-4
optimizer:
    # adam_eps: !!float 1e-8
    patience: 10
    warmup: 10
    clip: 1.0
    factor: 0.5
dataset:
    root_dir: null
    train:
        batch_size: 256
        shuffle: True
        clip_grads: True
    val:
        batch_size: 2048
